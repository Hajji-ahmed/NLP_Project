# ğŸš€ DÃ©ploiement de LLM avec Qwen2.5-0.5B, FastAPI et Gradio

Projet complet de dÃ©ploiement d'un modÃ¨le de langage (LLM) open-source avec API et interface web utilisant le modÃ¨le **Qwen2.5-0.5B-Instruct** d'Alibaba Cloud.

## ğŸ“ Structure du projet

```
NLP_Project/
â”‚â”€â”€ model/            # ModÃ¨le Qwen2.5-0.5B-Instruct tÃ©lÃ©chargÃ© ici
â”‚â”€â”€ api/
â”‚   â””â”€â”€ main.py       # API FastAPI
â”‚â”€â”€ web/
â”‚   â””â”€â”€ ui.py         # Interface web Gradio
â”‚â”€â”€ logs/
â”‚   â””â”€â”€ requests.log  # Logs des requÃªtes
â”‚â”€â”€ bench/
â”‚   â””â”€â”€ load_test.py  # Test de latence / concurrence
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ download_model.py # Script de tÃ©lÃ©chargement du modÃ¨le
â””â”€â”€ README.md
```

## ğŸ“ Ã€ propos du modÃ¨le

**Qwen2.5-0.5B-Instruct** est un modÃ¨le de langage compact (500M paramÃ¨tres) dÃ©veloppÃ© par Alibaba Cloud, optimisÃ© pour les tÃ¢ches d'instruction et de conversation. Ce modÃ¨le offre un excellent compromis entre performance et lÃ©gÃ¨retÃ©.

## âœ… Ã‰tape 1 â€” Installation des dÃ©pendances

```bash
pip install -r requirements.txt
```

## âœ… Ã‰tape 2 â€” TÃ©lÃ©charger le modÃ¨le Qwen2.5-0.5B-Instruct

```bash
python download_model.py
```

Cette commande tÃ©lÃ©charge automatiquement le modÃ¨le Qwen2.5-0.5B-Instruct depuis Hugging Face et le sauvegarde dans le dossier `model/`.

## âœ… Ã‰tape 3 â€” Lancer l'API FastAPI

```bash
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```

L'API est maintenant disponible sur `http://localhost:8000`

### Test de l'API

Vous pouvez tester l'endpoint `/generate` avec curl ou un client HTTP :

```bash
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello, how are you?", "max_new_tokens": 50}'
```

## âœ… Ã‰tape 4 â€” Lancer l'interface web Gradio

Dans un nouveau terminal :

```bash
python web/ui.py
```

Une page web s'ouvre automatiquement avec une interface simple :
- Zone de texte pour saisir votre prompt
- Le modÃ¨le gÃ©nÃ¨re une rÃ©ponse
- Interface type ChatGPT simplifiÃ©e

## âœ… Ã‰tape 5 â€” Tester la concurrence et la latence

```bash
python bench/load_test.py
```

Ce script envoie 100 requÃªtes simultanÃ©es et affiche la latence moyenne.

## âœ… Ã‰tape 6 â€” VÃ©rifier les logs

Tous les logs sont enregistrÃ©s dans :

```
logs/requests.log
```

Exemple de log :
```
2025-12-06 11:12:30 - INPUT from 127.0.0.1: Bonjour...
2025-12-06 11:12:30 - OUTPUT to 127.0.0.1: latency=0.156s
```

## ğŸ§  RÃ©sultat final

| Partie                        | Statut |
|-------------------------------|--------|
| Charger Qwen2.5-0.5B-Instruct | âœ”ï¸     |
| CrÃ©er API FastAPI             | âœ”ï¸     |
| Endpoint /generate            | âœ”ï¸     |
| Interface web Gradio          | âœ”ï¸     |
| Test concurrent & latence     | âœ”ï¸     |
| Logging complet               | âœ”ï¸     |

## ğŸ“Š CaractÃ©ristiques du modÃ¨le

- **ModÃ¨le** : Qwen2.5-0.5B-Instruct
- **ParamÃ¨tres** : 500M
- **DÃ©veloppeur** : Alibaba Cloud
- **Type** : ModÃ¨le instruction-tuned pour la gÃ©nÃ©ration de texte et la conversation
- **Avantages** : Compact, rapide, optimisÃ© pour les instructions

## ğŸ“ Notes

- **CPU vs GPU** : Le modÃ¨le dÃ©tecte automatiquement si CUDA est disponible
- **Latence** : Environ 0.1-0.5s par requÃªte selon le matÃ©riel
- **Concurrence** : FastAPI gÃ¨re naturellement les requÃªtes asynchrones
- **ModÃ¨le** : DistilGPT-2 est un modÃ¨le lÃ©ger (82M paramÃ¨tres) parfait pour un workshop

## ğŸ¯ Utilisation pour un workshop

Ce projet est prÃªt pour une dÃ©monstration complÃ¨te de :
1. Chargement d'un modÃ¨le Hugging Face
2. CrÃ©ation d'une API REST
3. Interface utilisateur web
4. Tests de performance
5. Monitoring avec logs

Bon workshop ! ğŸ‰
