# ğŸš€ DÃ©ploiement SÃ©curisÃ© de LLM avec Qwen2.5-0.5B, FastAPI et Gradio

Projet complet de dÃ©ploiement d'un modÃ¨le de langage (LLM) open-source avec une API FastAPI sÃ©curisÃ©e et une interface web Gradio. Utilise le modÃ¨le **Qwen2.5-0.5B-Instruct** d'Alibaba Cloud avec un systÃ¨me de filtrage de sÃ©curitÃ© **Llama-Guard-3-1B**.

## ğŸ“ Structure du projet

```
NLP_Project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py              # API FastAPI avec authentification
â”‚   â”‚   â”œâ”€â”€ llm_engine.py         # Moteur LLM (llama-cpp-python)
â”‚   â”‚   â””â”€â”€ security.py           # Moteur de sÃ©curitÃ© (filtrage + dÃ©tection)
â”‚   â”œâ”€â”€ logs/                     # Logs des requÃªtes API
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ qwen2.5-0.5b-instruct-q4_k_m.gguf   (Ã€ tÃ©lÃ©charger)
â”‚       â””â”€â”€ llama-guard-3-1b-q4_k_m.gguf        (Ã€ tÃ©lÃ©charger)
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ ui.py                     # Interface web Gradio
â”œâ”€â”€ bench/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ load_test.py              # Test de performance (latence / concurrence)
â”œâ”€â”€ requirements.txt              # DÃ©pendances Python
â””â”€â”€ README.md
```

## ğŸ“ Ã€ propos des modÃ¨les

- **Qwen2.5-0.5B-Instruct** : ModÃ¨le de langage compact (500M paramÃ¨tres) dÃ©veloppÃ© par Alibaba Cloud, optimisÃ© pour les tÃ¢ches conversationnelles
- **Llama-Guard-3-1B** : ModÃ¨le de sÃ©curitÃ© pour dÃ©tecter et filtrer les contenus potentiellement dangereux

## âœ… Ã‰tape 1 â€” Installation des dÃ©pendances

```bash
pip install -r requirements.txt
```

## âœ… Ã‰tape 2 â€” TÃ©lÃ©charger les modÃ¨les manuellement

Les modÃ¨les doivent Ãªtre tÃ©lÃ©chargÃ©s depuis Hugging Face et placÃ©s dans le dossier `backend/models/`.

### TÃ©lÃ©charger Qwen2.5-0.5B-Instruct
```bash
# Option 1: Avec huggingface_hub
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct-GGUF qwen2.5-0.5b-instruct-q4_k_m.gguf --local-dir backend/models/
```

Ou visitez manuellement: https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF

### TÃ©lÃ©charger Llama-Guard-3-1B
```bash
# Option 2: Avec huggingface_hub
huggingface-cli download meta-llama/Llama-Guard-3-1b-GGUF llama-guard-3-1b-q4_k_m.gguf --local-dir backend/models/
```

Ou visitez manuellement: https://huggingface.co/meta-llama/Llama-Guard-3-1b-GGUF

**VÃ©rifiez que les fichiers sont dans `backend/models/` avec les noms exacts :**
```
backend/models/
â”œâ”€â”€ qwen2.5-0.5b-instruct-q4_k_m.gguf
â””â”€â”€ llama-guard-3-1b-q4_k_m.gguf
```

## âœ… Ã‰tape 3 â€” Configurer la clÃ© API

Pour lancer l'API, vous devez dÃ©finir une variable d'environnement `LLM_API_KEY` :

```bash
# Windows (PowerShell)
$env:LLM_API_KEY="votre-clÃ©-secrÃ¨te"

# Windows (CMD)
set LLM_API_KEY=votre-clÃ©-secrÃ¨te

# Linux/Mac
export LLM_API_KEY="votre-clÃ©-secrÃ¨te"
```

## âœ… Ã‰tape 4 â€” Lancer l'API FastAPI

```bash
cd backend
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```

L'API est maintenant disponible sur `http://localhost:8000`

### Documentation interactive de l'API
- Swagger UI : http://localhost:8000/docs
- ReDoc : http://localhost:8000/redoc

### Test de l'API

```bash
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -H "X-API-Key: votre-clÃ©-secrÃ¨te" \
  -d '{"text": "Hello, how are you?", "max_new_tokens": 50}'
```

## âœ… Ã‰tape 5 â€” Lancer l'interface web Gradio

Dans un nouveau terminal :

```bash
python web/ui.py
```

Une interface web s'ouvre automatiquement avec :
- Zone de texte pour saisir votre prompt
- GÃ©nÃ©ration de rÃ©ponses en temps rÃ©el
- Interface intuitive type ChatGPT

### Configuration pour accÃ¨s Ã  distance

Pour accÃ©der Ã  l'API depuis un autre ordinateur, utilisez **Ngrok** :
```bash
ngrok http 8000
```

Mettez Ã  jour l'URL dans `web/ui.py` et `bench/load_test.py` avec l'URL Ngrok gÃ©nÃ©rÃ©.

## âœ… Ã‰tape 6 â€” Tester la performance et la concurrence

```bash
cd bench
python load_test.py
```

Ce script envoie 20 requÃªtes simultanÃ©es et affiche :
- Temps de rÃ©ponse pour chaque requÃªte
- Latence moyenne
- DÃ©tection des contenus bloquÃ©s

## ğŸ›¡ï¸ SystÃ¨me de sÃ©curitÃ© en 3 couches

L'API implÃ©mente une dÃ©fense en profondeur :

1. **Couche 1 - Regex** : Filtrage des patterns dangereux courants
2. **Couche 1.5 - DeBERTa** : DÃ©tection des injections de prompts
3. **Couche 2 - Llama-Guard** : Classification de sÃ©curitÃ© avec le modÃ¨le dÃ©diÃ©
4. **Couche 3 - XML Escaping** : Neutralisation des caractÃ¨res spÃ©ciaux

## ğŸ“Š Logs et monitoring

Tous les logs sont enregistrÃ©s dans `backend/logs/requests.log` :

```
2025-12-20 14:32:15 - INPUT from 127.0.0.1: "What is AI?"
2025-12-20 14:32:16 - OUTPUT to 127.0.0.1: latency=1.234s | tokens=45
2025-12-20 14:32:18 - BLOCKED L2: Dangerous content detected
```

## ğŸ“¦ DÃ©pendances principales

- **FastAPI** : Framework web asynchrone haute performance
- **Uvicorn** : Serveur ASGI
- **llama-cpp-python** : ExÃ©cution de modÃ¨les GGUF en Python
- **Gradio** : CrÃ©ation d'interfaces web sans code
- **Transformers** : ModÃ¨les NLP de Hugging Face
- **Torch** : Framework deep learning
- **python-dotenv** : Gestion des variables d'environnement
- **Pydantic** : Validation de donnÃ©es

## ğŸ§  CaractÃ©ristiques

| FonctionnalitÃ©              | Statut |
|-----------------------------|--------|
| Chargement Qwen2.5-0.5B     | âœ”ï¸     |
| Chargement Llama-Guard-3-1B | âœ”ï¸     |
| API FastAPI sÃ©curisÃ©e       | âœ”ï¸     |
| Endpoint `/generate`        | âœ”ï¸     |
| Authentification API Key     | âœ”ï¸     |
| Filtrage de sÃ©curitÃ©        | âœ”ï¸     |
| Interface Gradio            | âœ”ï¸     |
| Tests de performance        | âœ”ï¸     |
| Logs des requÃªtes           | âœ”ï¸     |

## ğŸ”— Ressources

- [Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF)
- [Llama-Guard-3-1B](https://huggingface.co/meta-llama/Llama-Guard-3-1b-GGUF)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Gradio Documentation](https://www.gradio.app/)
| Interface web Gradio          | âœ”ï¸     |
| Test concurrent & latence     | âœ”ï¸     |
| Logging complet               | âœ”ï¸     |

## ğŸ“Š CaractÃ©ristiques du modÃ¨le

- **ModÃ¨le** : Qwen2.5-0.5B-Instruct
- **ParamÃ¨tres** : 500M
- **DÃ©veloppeur** : Alibaba Cloud
- **Type** : ModÃ¨le instruction-tuned pour la gÃ©nÃ©ration de texte et la conversation
- **Avantages** : Compact, rapide, optimisÃ© pour les instructions

## ğŸ“ Notes

- **CPU vs GPU** : Le modÃ¨le dÃ©tecte automatiquement si CUDA est disponible
- **Latence** : Environ 0.1-0.5s par requÃªte selon le matÃ©riel
- **Concurrence** : FastAPI gÃ¨re naturellement les requÃªtes asynchrones
- **ModÃ¨le** : DistilGPT-2 est un modÃ¨le lÃ©ger (82M paramÃ¨tres) parfait pour un workshop

